{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paraphrasing Sentences Optimize Classification Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Language: Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from random import choice\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import html\n",
    "import string\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yurui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yurui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Is_Unreliable</th>\n",
       "      <th>Category</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1, 3, 6, 9</td>\n",
       "      <td>We are living in scary times in Canada. Gov’t ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1, 6, 8, 9</td>\n",
       "      <td>Just as bad in Canada. In fact, our government...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1, 4, 9</td>\n",
       "      <td>It was only a matter of time before the mainst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6, 8</td>\n",
       "      <td>Russia's taking no chances: Foreigners infecte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>6, 8, 9</td>\n",
       "      <td>Although there is now a presumptive confirmed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BREAKING: Harvard classes will move online sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Singularity University is hosting a FREE Virtu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Coronavirus: how does it spread and what are t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Stanford just cancelled classes for the rest o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tech conferences were cancelled in #Waterloo R...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>560 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Is_Unreliable    Category  \\\n",
       "0                1  1, 3, 6, 9   \n",
       "1                1  1, 6, 8, 9   \n",
       "2                1     1, 4, 9   \n",
       "3                1        6, 8   \n",
       "4                1     6, 8, 9   \n",
       "..             ...         ...   \n",
       "555              0         NaN   \n",
       "556              0         NaN   \n",
       "557              0         NaN   \n",
       "558              0         NaN   \n",
       "559              0         NaN   \n",
       "\n",
       "                                                 Tweet  \n",
       "0    We are living in scary times in Canada. Gov’t ...  \n",
       "1    Just as bad in Canada. In fact, our government...  \n",
       "2    It was only a matter of time before the mainst...  \n",
       "3    Russia's taking no chances: Foreigners infecte...  \n",
       "4    Although there is now a presumptive confirmed ...  \n",
       "..                                                 ...  \n",
       "555  BREAKING: Harvard classes will move online sta...  \n",
       "556  Singularity University is hosting a FREE Virtu...  \n",
       "557  Coronavirus: how does it spread and what are t...  \n",
       "558  Stanford just cancelled classes for the rest o...  \n",
       "559  Tech conferences were cancelled in #Waterloo R...  \n",
       "\n",
       "[560 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.mode.chained_assignment = None # deal with SettingWithCopyWarning\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "stopwords_nltk = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "Data = pd.read_csv(\"COVID19_Dataset.csv\")\n",
    "Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing tweets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove useless words from tweets (Ex: url, digitis, punctuations, @__)\n",
    "- Use <b>lemmatization</b> to returns an actual word of the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocessing\n",
    "Data['clean_tweet'] = Data['Tweet']\n",
    "\n",
    "clean_text = \"\"\n",
    "for i in range(0, Data['Tweet'].shape[0]):\n",
    "    text = Data['Tweet'].iloc[i].lower() # lowercase\n",
    "    text = re.sub(r'http\\S+','', text) # remove url\n",
    "    text = html.unescape(text) # convert XML to characters\n",
    "    text = re.sub(r\"[^A-z@]\", \" \", text) # remove digits,punctuations except @\n",
    "    text = re.sub(r'@\\S+','', text) # remove @__words\n",
    "    text = ' '.join([token for token in text.split()])\n",
    "    \n",
    "   # clean stopwords\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    tokenized_text_without_stopwords = [k for k in tokenized_text if not k in stopwords_nltk] \n",
    "   \n",
    "    # lemmatization\n",
    "    tokenized_text_without_stopwords_lem = [] \n",
    "    for j in range(0, len(tokenized_text_without_stopwords)):\n",
    "        tokenized_text_without_stopwords_lem.append(lemmatizer.lemmatize(tokenized_text_without_stopwords[j]))\n",
    "    \n",
    "    tokenized_text_without_stopwords_lem = ' '.join(tokenized_text_without_stopwords_lem)\n",
    "    clean_text = tokenized_text_without_stopwords_lem\n",
    "    Data['clean_tweet'].iloc[i] = clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit on classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use <b>TF-IDF</b> to vectorize words.\n",
    "- Fit on <b>Logistic Regression</b> and <b>SVM</b> to classify tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1.\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = Data['clean_tweet']\n",
    "y = Data['Is_Unreliable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(lowercase = True,\n",
    "                        stop_words = 'english',\n",
    "                        ngram_range = (1,1))\n",
    "\n",
    "X_dtm = tfidf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf Logistic Regression Accuracy with 10-folds validation:  0.812\n"
     ]
    }
   ],
   "source": [
    "# tfidf-logistic\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression #import\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "logreg = LogisticRegression(class_weight=\"balanced\") #instantiate a logistic regression model\n",
    "scores = cross_val_score(logreg, X_dtm, y, cv= 10) # 10 folds validation\n",
    "print( \"tfidf Logistic Regression Accuracy with 10-folds validation: \", round(np.mean(scores),3))\n",
    "\n",
    "a = (\"tfidf Logistic Regression Accuracy: \"+ str(round(np.mean(scores),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pipeline for SVC\n",
    "- Automated words vetorization & SVM to get accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(probability=True)\n",
    "pipe = Pipeline([\n",
    "('vectorize', tfidf),\n",
    "('classify', SVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = ['rbf', 'linear', 'poly', 'sigmoid']\n",
    "C = [0.001, 0.01, 0.1, 1, 10]\n",
    "# set up parameter grid\n",
    "params = {\n",
    "'classify__kernel': kernel,\n",
    "'classify__C': C\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, KFold, GridSearchCV\n",
    "\n",
    "inner_cv = KFold(n_splits = 3, shuffle = True, random_state = 1)\n",
    "outer_cv = KFold(n_splits = 10, shuffle = True, random_state = 1)\n",
    "\n",
    "# Set up GridSearch for inner loop\n",
    "grid_SVC = GridSearchCV(pipe, params, cv = inner_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(grid_SVC,\n",
    "                        X = X,\n",
    "                        y = y,\n",
    "                        cv = outer_cv,\n",
    "                        scoring = ['roc_auc', 'accuracy', 'f1', 'precision', 'recall'],\n",
    "                        return_estimator = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.83928571 0.78571429 0.80357143 0.82142857 0.875      0.78571429\n",
      " 0.76785714 0.89285714 0.82142857 0.80357143]\n",
      "\n",
      "tfidf SVC Accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "accuracy = scores['test_accuracy']\n",
    "print(accuracy)\n",
    "b = (\"tfidf SVC Accuracy: \"+ str(round(accuracy.mean(),3)))\n",
    "print(\"\\n\"+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parapharse tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- paraphrase 5 sentences for each train data tweet to expand dataset.\n",
    "- Use nltk module.\n",
    "- Check synonyms words and replace original words to create new sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2.\n",
    "## paraphrase 5 sentences for each train data tweet (560*0.7 + 560*0.7*5)##\n",
    "\n",
    "X = Data['clean_tweet']\n",
    "y = Data['Is_Unreliable']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,train_size= 0.7, random_state=1) # 70%/ 30%\n",
    "\n",
    "all_train_data = y_train.to_frame().join(X_train.to_frame()) # 392\n",
    "\n",
    "new_train_data = all_train_data.append(all_train_data) # 392+392 = 784\n",
    "new_train_data = new_train_data.append(new_train_data) # 784+784 = 1568\n",
    "new_train_data = new_train_data.append(all_train_data) # 1568+392 = 1960\n",
    "new_train_data = new_train_data.append(all_train_data) # 1960+392 = 2352\n",
    "\n",
    "for i in range(0,len(all_train_data)):\n",
    "     new_train_data.iloc[i*5] = all_train_data.iloc[i]\n",
    "     new_train_data.iloc[i*5+1] = all_train_data.iloc[i]\n",
    "     new_train_data.iloc[i*5+2] = all_train_data.iloc[i]\n",
    "     new_train_data.iloc[i*5+3] = all_train_data.iloc[i]\n",
    "     new_train_data.iloc[i*5+4] = all_train_data.iloc[i]\n",
    "     new_train_data.iloc[i*5+5] = all_train_data.iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paraphrase def #\n",
    "\n",
    "def tag(sentence): # tag each word's part of speech\n",
    " words = word_tokenize(sentence)\n",
    " words = pos_tag(words)\n",
    " return words\n",
    "\n",
    "def paraphraseable(tag): \n",
    " return tag.startswith('NN') or tag == 'VB' or tag.startswith('JJ')\n",
    "\n",
    "def pos(tag):\n",
    " if tag.startswith('NN'):\n",
    "  return wn.NOUN\n",
    " elif tag.startswith('V'):\n",
    "  return wn.VERB\n",
    "\n",
    "def synonyms(word, tag):\n",
    "    lemma_lists = [ss.lemmas() for ss in wn.synsets(word, pos(tag))]\n",
    "    lemmas = [lemma.name() for lemma in sum(lemma_lists, [])]\n",
    "    return set(lemmas)\n",
    "\n",
    "def synonymIfExists(sentence):\n",
    " for (word, t) in tag(sentence):\n",
    "   if paraphraseable(t):\n",
    "    syns = synonyms(word, t)\n",
    "    if syns:\n",
    "     if len(syns) > 1:\n",
    "      yield [word, list(syns)]\n",
    "      continue\n",
    "   yield [word, []]\n",
    "\n",
    "def paraphrase(sentence):\n",
    " return [x for x in synonymIfExists(sentence)]\n",
    "\n",
    "def generator(sentence):\n",
    "    sentence = paraphrase(sentence)\n",
    "    text = list(range(len(sentence)))\n",
    "    for i in range(0, len(sentence)):\n",
    "        if sentence[i][1] == []:\n",
    "            text[i] = sentence[i][0]\n",
    "        else:\n",
    "            text[i] = choice(sentence[i][1])\n",
    "        \n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add paraphrasing 3 sentences for each tweet \n",
    "for i in range(0,len(all_train_data)):\n",
    "    new_train_data.iloc[i*5+1,1] = generator(all_train_data.iloc[i,1])\n",
    "    new_train_data.iloc[i*5+2,1] = generator(all_train_data.iloc[i,1])\n",
    "    new_train_data.iloc[i*5+3,1] = generator(all_train_data.iloc[i,1])\n",
    "    new_train_data.iloc[i*5+4,1] = generator(all_train_data.iloc[i,1])\n",
    "    new_train_data.iloc[i*5+5,1] = generator(all_train_data.iloc[i,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model accuracy again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use TF-IDF vectorize again\n",
    "- Fit Logistic and SVM to check accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3.\n",
    "# use new train data(1568 tweets, including original and paraphrase)\n",
    "\n",
    "new_test_data = y_test.to_frame().join(X_test.to_frame())\n",
    "all_new_data = new_train_data.append(new_test_data)\n",
    "\n",
    "X = all_new_data['clean_tweet']\n",
    "y = all_new_data['Is_Unreliable']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf\n",
    "tfidf = TfidfVectorizer(lowercase = True,\n",
    "                        stop_words = 'english',\n",
    "                        ngram_range = (1,1))\n",
    "\n",
    "\n",
    "X_dtm = tfidf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf Logistic Regression Accuracy with 10-folds validation:  0.839\n"
     ]
    }
   ],
   "source": [
    "# tfidf-logistic\n",
    "logreg = LogisticRegression(class_weight=\"balanced\") #instantiate a logistic regression model\n",
    "scores = cross_val_score(logreg, X_dtm, y, cv= 10) # 10 folds validation\n",
    "print( \"tfidf Logistic Regression Accuracy with 10-folds validation: \", round(np.mean(scores),3))\n",
    "\n",
    "a2 = (\"After paraphrasing, tfidf Logistic Regression Accuracy: \"+ str(round(np.mean(scores),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC\n",
    "svc = SVC(probability=True)\n",
    "pipe = Pipeline([\n",
    "('vectorize', tfidf),\n",
    "('classify', SVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = ['rbf', 'linear', 'poly', 'sigmoid']\n",
    "C = [0.001, 0.01, 0.1, 1, 10]\n",
    "# set up parameter grid\n",
    "params = {\n",
    "'classify__kernel': kernel,\n",
    "'classify__C': C\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_cv = KFold(n_splits = 3, shuffle = True, random_state = 1)\n",
    "outer_cv = KFold(n_splits = 10, shuffle = True, random_state = 1)\n",
    "\n",
    "# Set up GridSearch for inner loop\n",
    "grid_SVC = GridSearchCV(pipe, params, cv = inner_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(grid_SVC,\n",
    "                        X = X,\n",
    "                        y = y,\n",
    "                        cv = outer_cv,\n",
    "                        scoring = ['roc_auc', 'accuracy', 'f1', 'precision', 'recall'],\n",
    "                        return_estimator = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.91269841 0.86904762 0.90079365 0.92460317 0.93253968 0.92063492\n",
      " 0.91666667 0.92857143 0.92063492 0.88492063]\n",
      "\n",
      "After paraphrasing, tfidf SVC Accuracy: 0.911\n"
     ]
    }
   ],
   "source": [
    "accuracy = scores['test_accuracy']\n",
    "print(accuracy)\n",
    "b2 = (\"After paraphrasing, tfidf SVC Accuracy: \" +str(round(accuracy.mean(),3)))\n",
    "print(\"\\n\"+ b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf Logistic Regression Accuracy: 0.812\n",
      "After paraphrasing, tfidf Logistic Regression Accuracy: 0.839\n",
      "\n",
      "tfidf SVC Accuracy: 0.82\n",
      "After paraphrasing, tfidf SVC Accuracy: 0.911\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(a2 + \"\\n\")\n",
    "\n",
    "print(b)\n",
    "print(b2 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Indeed, our accuracy get better after we parapharse our tweets to expand dataset. Logistic from 81.2% to 83.9%; SVM from 82% to 91%. However, if we want to get more better accuracy, we probably need to create new sentence with different structure. Because I only change words on my work, the structure is still same. If we change the words and structure to create new sentences, the training model can learn more information and get more improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
