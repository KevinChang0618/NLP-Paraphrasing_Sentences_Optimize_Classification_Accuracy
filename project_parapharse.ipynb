{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paraphrasing Sentences Optimize Classification Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Language: Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from random import choice\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import html\n",
    "import string\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\n",
    "from keras.models import Model, Sequential\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yurui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yurui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Is_Unreliable</th>\n",
       "      <th>Category</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1, 3, 6, 9</td>\n",
       "      <td>We are living in scary times in Canada. Gov’t ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1, 6, 8, 9</td>\n",
       "      <td>Just as bad in Canada. In fact, our government...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1, 4, 9</td>\n",
       "      <td>It was only a matter of time before the mainst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6, 8</td>\n",
       "      <td>Russia's taking no chances: Foreigners infecte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>6, 8, 9</td>\n",
       "      <td>Although there is now a presumptive confirmed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BREAKING: Harvard classes will move online sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Singularity University is hosting a FREE Virtu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Coronavirus: how does it spread and what are t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Stanford just cancelled classes for the rest o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tech conferences were cancelled in #Waterloo R...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>560 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Is_Unreliable    Category  \\\n",
       "0                1  1, 3, 6, 9   \n",
       "1                1  1, 6, 8, 9   \n",
       "2                1     1, 4, 9   \n",
       "3                1        6, 8   \n",
       "4                1     6, 8, 9   \n",
       "..             ...         ...   \n",
       "555              0         NaN   \n",
       "556              0         NaN   \n",
       "557              0         NaN   \n",
       "558              0         NaN   \n",
       "559              0         NaN   \n",
       "\n",
       "                                                 Tweet  \n",
       "0    We are living in scary times in Canada. Gov’t ...  \n",
       "1    Just as bad in Canada. In fact, our government...  \n",
       "2    It was only a matter of time before the mainst...  \n",
       "3    Russia's taking no chances: Foreigners infecte...  \n",
       "4    Although there is now a presumptive confirmed ...  \n",
       "..                                                 ...  \n",
       "555  BREAKING: Harvard classes will move online sta...  \n",
       "556  Singularity University is hosting a FREE Virtu...  \n",
       "557  Coronavirus: how does it spread and what are t...  \n",
       "558  Stanford just cancelled classes for the rest o...  \n",
       "559  Tech conferences were cancelled in #Waterloo R...  \n",
       "\n",
       "[560 rows x 3 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.mode.chained_assignment = None # deal with SettingWithCopyWarning\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "stopwords_nltk = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "Data = pd.read_csv(\"COVID19_Dataset.csv\")\n",
    "Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing tweets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove useless words from tweets (Ex: url, digitis, punctuations, @__)\n",
    "- Use <b>lemmatization</b> to returns an actual word of the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocessing\n",
    "Data['clean_tweet'] = Data['Tweet']\n",
    "\n",
    "clean_text = \"\"\n",
    "for i in range(0, Data['Tweet'].shape[0]):\n",
    "    text = Data['Tweet'].iloc[i].lower() # lowercase\n",
    "    text = re.sub(r'http\\S+','', text) # remove url\n",
    "    text = html.unescape(text) # convert XML to characters\n",
    "    text = re.sub(r\"[^A-z@]\", \" \", text) # remove digits,punctuations except @\n",
    "    text = re.sub(r'@\\S+','', text) # remove @__words\n",
    "    text = ' '.join([token for token in text.split()])\n",
    "    \n",
    "   # clean stopwords\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    tokenized_text_without_stopwords = [k for k in tokenized_text if not k in stopwords_nltk] \n",
    "   \n",
    "    # lemmatization\n",
    "    tokenized_text_without_stopwords_lem = [] \n",
    "    for j in range(0, len(tokenized_text_without_stopwords)):\n",
    "        tokenized_text_without_stopwords_lem.append(lemmatizer.lemmatize(tokenized_text_without_stopwords[j]))\n",
    "    \n",
    "    tokenized_text_without_stopwords_lem = ' '.join(tokenized_text_without_stopwords_lem)\n",
    "    clean_text = tokenized_text_without_stopwords_lem\n",
    "    Data['clean_tweet'].iloc[i] = clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use CNN to classify tweets reliable/ unreliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B') # source: https://nlp.stanford.edu/projects/glove/\n",
    "                                               # using on embedding\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "MAX_NUM_WORDS = 20000 \n",
    "EMBEDDING_DIM = 100 \n",
    "VALIDATION_SPLIT = 0.2 # 20% for testing 80% for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tweets = Data['clean_tweet']\n",
    "labels = Data['Is_Unreliable']\n",
    "labels_index = {'fake':1, 'real':0}\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(tweets, labels, train_size= 0.7, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1784 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "#Vectorize these text samples into a 2D integer tensor using Keras Tokenizer\n",
    "#Tokenizer is fit on training data only, and that is used to tokenize both train and test data.\n",
    "tokenizer = Tokenizer(num_words= MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_texts) # Converting text to a vector of word indices\n",
    "                                                            # the words indices for each sentence\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "\n",
    "word_index = tokenizer.word_index # words from all docs\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the train data into train and valid is done\n"
     ]
    }
   ],
   "source": [
    "# initial padding of 0s, until vector is of size MAX_SEQUENCE_LENGTH\n",
    "trainvalid_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "trainvalid_labels = to_categorical(train_labels, num_classes = 2, dtype =\"int32\")\n",
    "test_labels = to_categorical(test_labels, num_classes = 2, dtype =\"int32\")\n",
    "\n",
    "# split the training data into a training set and a validation set\n",
    "indices = np.arange(trainvalid_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "trainvalid_data = trainvalid_data[indices]\n",
    "trainvalid_labels = trainvalid_labels[indices]\n",
    "\n",
    "num_validation_samples = int(VALIDATION_SPLIT * trainvalid_data.shape[0])\n",
    "\n",
    "x_train = trainvalid_data[ :-num_validation_samples] # not include num_validation_samples\n",
    "y_train = trainvalid_labels[ :-num_validation_samples]\n",
    "\n",
    "x_val = trainvalid_data[-num_validation_samples: ] # only num_validation_samples\n",
    "y_val = trainvalid_labels[-num_validation_samples: ]\n",
    "\n",
    "#This is the data we will use for CNN and RNN training\n",
    "print('Splitting the train data into train and valid is done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n",
      "Found 400000 word vectors in Glove embeddings.\n",
      "Preparing of embedding matrix is done\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "# first, build index mapping words in the embeddings set to their embedding vector\n",
    "embeddings_index = {}\n",
    "\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'),encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors in Glove embeddings.' % len(embeddings_index))\n",
    "#print(embeddings_index[\"google\"]) # each word has 100 dims\n",
    "\n",
    "# prepare embedding matrix - rows are the words from word_index, columns are the embeddings of that word from glove.\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1 # 13831 + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM)) # 13832(words)x 100(dim)\n",
    "\n",
    "for word, i in word_index.items(): # key,value in dictionary.items()\n",
    "    if i > MAX_NUM_WORDS:          # if word corresponding value > MAX_NUM_WORDS(20000) would ignore below function\n",
    "        continue                   # do next word.\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:          # words not found in embedding index will be all-zeros.\n",
    "       embedding_matrix[i] = embedding_vector # every words would have own vectors.\n",
    "\n",
    "# load these pre-trained word embeddings into an Embedding layer\n",
    "# note that we set 'trainable = False' so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,     # 13832\n",
    "                            EMBEDDING_DIM, # 100 (channel)\n",
    "                            embeddings_initializer= Constant(embedding_matrix),\n",
    "                            input_length= MAX_SEQUENCE_LENGTH, # 500 (width)\n",
    "                            trainable=False) # don't need weight for words\n",
    "print(\"Preparing of embedding matrix is done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Define a 1D CNN model.\n"
     ]
    }
   ],
   "source": [
    "print('Define a 1D CNN model.')\n",
    "\n",
    "cnnmodel = Sequential()\n",
    "cnnmodel.add(embedding_layer)                   # input layer: 500x100\n",
    "\n",
    "cnnmodel.add(Conv1D(128, 5, activation='relu')) # 128 output space; filter size=5; activation function\n",
    "cnnmodel.add(MaxPooling1D(5))                   # max pool size = 5 (maximum value of window size)\n",
    "cnnmodel.add(Dropout(0.25))                     # may avoid overfitting\n",
    "\n",
    "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
    "cnnmodel.add(MaxPooling1D(5))\n",
    "cnnmodel.add(Dropout(0.25))\n",
    "\n",
    "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
    "cnnmodel.add(GlobalMaxPooling1D())\n",
    "cnnmodel.add(Dense(128, activation='relu'))\n",
    "cnnmodel.add(Dropout(0.25))\n",
    "\n",
    "cnnmodel.add(Dense(len(labels_index), activation='softmax')) # classify labels 0 or 1, so is 2; 'softmax' get probability\n",
    "\n",
    "cnnmodel.compile(loss='categorical_crossentropy', # loss function\n",
    "                 optimizer='Adam',\n",
    "                 metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(314, 500)\n",
      "(314, 2)\n",
      "Epoch 1/5\n",
      "10/10 [==============================] - 2s 143ms/step - loss: 0.6916 - acc: 0.5026 - val_loss: 0.6850 - val_acc: 0.4872\n",
      "Epoch 2/5\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 0.6657 - acc: 0.5928 - val_loss: 0.6347 - val_acc: 0.6667\n",
      "Epoch 3/5\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 0.5826 - acc: 0.8101 - val_loss: 0.5255 - val_acc: 0.7949\n",
      "Epoch 4/5\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 0.4279 - acc: 0.8296 - val_loss: 0.7444 - val_acc: 0.6667\n",
      "Epoch 5/5\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 0.3924 - acc: 0.8349 - val_loss: 0.5081 - val_acc: 0.7308\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "cnn_train = cnnmodel.fit(x_train, y_train, # data, label\n",
    "                         batch_size= 32,   # each time for 32 samples for training set until all samples for 1 epoch.\n",
    "                         epochs = 5,  \n",
    "                         verbose = 1,      # return some informations during training\n",
    "                         validation_data= (x_val, y_val))\n",
    "\n",
    "# epoch , train how many time, low: underfitting, high: overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 25ms/step - loss: 0.2288 - acc: 0.9236\n",
      "Training accuracy with CNN: 0.9235668778419495\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.5264 - acc: 0.7321\n",
      "Testing accuracy with CNN: 0.7321428656578064\n"
     ]
    }
   ],
   "source": [
    "score_train, acc_train = cnnmodel.evaluate(x_train, y_train)\n",
    "print('Training accuracy with CNN:', acc_train)\n",
    "\n",
    "score_test, acc_test = cnnmodel.evaluate(test_data, test_labels)\n",
    "print('Testing accuracy with CNN:', acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit on classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use <b>TF-IDF</b> to vectorize words.\n",
    "- Fit on <b>Logistic Regression</b> and <b>SVM</b> to classify tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1.\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = Data['clean_tweet']\n",
    "y = Data['Is_Unreliable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(lowercase = True,\n",
    "                        stop_words = 'english',\n",
    "                        ngram_range = (1,1))\n",
    "\n",
    "X_dtm = tfidf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf Logistic Regression Accuracy with 10-folds validation:  0.812\n"
     ]
    }
   ],
   "source": [
    "# tfidf-logistic\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression #import\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "logreg = LogisticRegression(class_weight=\"balanced\") #instantiate a logistic regression model\n",
    "scores = cross_val_score(logreg, X_dtm, y, cv= 10) # 10 folds validation\n",
    "print( \"tfidf Logistic Regression Accuracy with 10-folds validation: \", round(np.mean(scores),3))\n",
    "\n",
    "a = (\"tfidf Logistic Regression Accuracy: \"+ str(round(np.mean(scores),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pipeline for SVC\n",
    "- Automated words vetorization & SVM to get accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(probability=True)\n",
    "pipe = Pipeline([\n",
    "('vectorize', tfidf),\n",
    "('classify', SVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = ['rbf', 'linear', 'poly', 'sigmoid']\n",
    "C = [0.001, 0.01, 0.1, 1, 10]\n",
    "# set up parameter grid\n",
    "params = {\n",
    "'classify__kernel': kernel,\n",
    "'classify__C': C\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, KFold, GridSearchCV\n",
    "\n",
    "inner_cv = KFold(n_splits = 3, shuffle = True, random_state = 1)\n",
    "outer_cv = KFold(n_splits = 10, shuffle = True, random_state = 1)\n",
    "\n",
    "# Set up GridSearch for inner loop\n",
    "grid_SVC = GridSearchCV(pipe, params, cv = inner_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(grid_SVC,\n",
    "                        X = X,\n",
    "                        y = y,\n",
    "                        cv = outer_cv,\n",
    "                        scoring = ['roc_auc', 'accuracy', 'f1', 'precision', 'recall'],\n",
    "                        return_estimator = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.83928571 0.78571429 0.80357143 0.82142857 0.875      0.78571429\n",
      " 0.76785714 0.89285714 0.82142857 0.80357143]\n",
      "\n",
      "tfidf SVC Accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "accuracy = scores['test_accuracy']\n",
    "print(accuracy)\n",
    "b = (\"tfidf SVC Accuracy: \"+ str(round(accuracy.mean(),3)))\n",
    "print(\"\\n\"+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parapharse tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- paraphrase 5 sentences for each train data tweet to expand dataset.\n",
    "- Use nltk module.\n",
    "- Check synonyms words and replace original words to create new sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2.\n",
    "## paraphrase 5 sentences for each train data tweet (560*0.7 + 560*0.7*5)##\n",
    "\n",
    "X = Data['clean_tweet']\n",
    "y = Data['Is_Unreliable']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,train_size= 0.7, random_state=1) # 70%/ 30%\n",
    "\n",
    "all_train_data = y_train.to_frame().join(X_train.to_frame()) # 392\n",
    "\n",
    "new_train_data = all_train_data.append(all_train_data) # 392+392 = 784\n",
    "new_train_data = new_train_data.append(new_train_data) # 784+784 = 1568\n",
    "new_train_data = new_train_data.append(all_train_data) # 1568+392 = 1960\n",
    "new_train_data = new_train_data.append(all_train_data) # 1960+392 = 2352\n",
    "\n",
    "for i in range(0,len(all_train_data)):\n",
    "     new_train_data.iloc[i*5] = all_train_data.iloc[i]\n",
    "     new_train_data.iloc[i*5+1] = all_train_data.iloc[i]\n",
    "     new_train_data.iloc[i*5+2] = all_train_data.iloc[i]\n",
    "     new_train_data.iloc[i*5+3] = all_train_data.iloc[i]\n",
    "     new_train_data.iloc[i*5+4] = all_train_data.iloc[i]\n",
    "     new_train_data.iloc[i*5+5] = all_train_data.iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paraphrase def #\n",
    "\n",
    "def tag(sentence): # tag each word's part of speech\n",
    " words = word_tokenize(sentence)\n",
    " words = pos_tag(words)\n",
    " return words\n",
    "\n",
    "def paraphraseable(tag): \n",
    " return tag.startswith('NN') or tag == 'VB' or tag.startswith('JJ')\n",
    "\n",
    "def pos(tag):\n",
    " if tag.startswith('NN'):\n",
    "  return wn.NOUN\n",
    " elif tag.startswith('V'):\n",
    "  return wn.VERB\n",
    "\n",
    "def synonyms(word, tag):\n",
    "    lemma_lists = [ss.lemmas() for ss in wn.synsets(word, pos(tag))]\n",
    "    lemmas = [lemma.name() for lemma in sum(lemma_lists, [])]\n",
    "    return set(lemmas)\n",
    "\n",
    "def synonymIfExists(sentence):\n",
    " for (word, t) in tag(sentence):\n",
    "   if paraphraseable(t):\n",
    "    syns = synonyms(word, t)\n",
    "    if syns:\n",
    "     if len(syns) > 1:\n",
    "      yield [word, list(syns)]\n",
    "      continue\n",
    "   yield [word, []]\n",
    "\n",
    "def paraphrase(sentence):\n",
    " return [x for x in synonymIfExists(sentence)]\n",
    "\n",
    "def generator(sentence):\n",
    "    sentence = paraphrase(sentence)\n",
    "    text = list(range(len(sentence)))\n",
    "    for i in range(0, len(sentence)):\n",
    "        if sentence[i][1] == []:\n",
    "            text[i] = sentence[i][0]\n",
    "        else:\n",
    "            text[i] = choice(sentence[i][1])\n",
    "        \n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add paraphrasing 3 sentences for each tweet \n",
    "for i in range(0,len(all_train_data)):\n",
    "    new_train_data.iloc[i*5+1,1] = generator(all_train_data.iloc[i,1])\n",
    "    new_train_data.iloc[i*5+2,1] = generator(all_train_data.iloc[i,1])\n",
    "    new_train_data.iloc[i*5+3,1] = generator(all_train_data.iloc[i,1])\n",
    "    new_train_data.iloc[i*5+4,1] = generator(all_train_data.iloc[i,1])\n",
    "    new_train_data.iloc[i*5+5,1] = generator(all_train_data.iloc[i,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try CNN again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_data = y_test.to_frame().join(X_test.to_frame())\n",
    "all_new_data = new_train_data.append(new_test_data)\n",
    "\n",
    "tweets = all_new_data['clean_tweet']\n",
    "labels = all_new_data['Is_Unreliable']\n",
    "labels_index = {'fake':1, 'real':0}\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(tweets, labels, train_size= 0.7, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4279 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "#Vectorize these text samples into a 2D integer tensor using Keras Tokenizer\n",
    "#Tokenizer is fit on training data only, and that is used to tokenize both train and test data.\n",
    "tokenizer = Tokenizer(num_words= MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_texts) # Converting text to a vector of word indices\n",
    "                                                            # the words indices for each sentence\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "\n",
    "word_index = tokenizer.word_index # words from all docs\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the train data into train and valid is done\n"
     ]
    }
   ],
   "source": [
    "# initial padding of 0s, until vector is of size MAX_SEQUENCE_LENGTH\n",
    "trainvalid_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "trainvalid_labels = to_categorical(train_labels, num_classes = 2, dtype =\"int32\")\n",
    "test_labels = to_categorical(test_labels, num_classes = 2, dtype =\"int32\")\n",
    "\n",
    "# split the training data into a training set and a validation set\n",
    "indices = np.arange(trainvalid_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "trainvalid_data = trainvalid_data[indices]\n",
    "trainvalid_labels = trainvalid_labels[indices]\n",
    "\n",
    "num_validation_samples = int(VALIDATION_SPLIT * trainvalid_data.shape[0])\n",
    "\n",
    "x_train = trainvalid_data[ :-num_validation_samples] # not include num_validation_samples\n",
    "y_train = trainvalid_labels[ :-num_validation_samples]\n",
    "\n",
    "x_val = trainvalid_data[-num_validation_samples: ] # only num_validation_samples\n",
    "y_val = trainvalid_labels[-num_validation_samples: ]\n",
    "\n",
    "#This is the data we will use for CNN and RNN training\n",
    "print('Splitting the train data into train and valid is done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n",
      "Found 400000 word vectors in Glove embeddings.\n",
      "Preparing of embedding matrix is done\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "# first, build index mapping words in the embeddings set to their embedding vector\n",
    "embeddings_index = {}\n",
    "\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'),encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors in Glove embeddings.' % len(embeddings_index))\n",
    "#print(embeddings_index[\"google\"]) # each word has 100 dims\n",
    "\n",
    "# prepare embedding matrix - rows are the words from word_index, columns are the embeddings of that word from glove.\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1 # 13831 + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM)) # 13832(words)x 100(dim)\n",
    "\n",
    "for word, i in word_index.items(): # key,value in dictionary.items()\n",
    "    if i > MAX_NUM_WORDS:          # if word corresponding value > MAX_NUM_WORDS(20000) would ignore below function\n",
    "        continue                   # do next word.\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:          # words not found in embedding index will be all-zeros.\n",
    "       embedding_matrix[i] = embedding_vector # every words would have own vectors.\n",
    "\n",
    "# load these pre-trained word embeddings into an Embedding layer\n",
    "# note that we set 'trainable = False' so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,     # 13832\n",
    "                            EMBEDDING_DIM, # 100 (channel)\n",
    "                            embeddings_initializer= Constant(embedding_matrix),\n",
    "                            input_length= MAX_SEQUENCE_LENGTH, # 500 (width)\n",
    "                            trainable=False) # don't need weight for words\n",
    "print(\"Preparing of embedding matrix is done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Define a 1D CNN model.\n"
     ]
    }
   ],
   "source": [
    "print('Define a 1D CNN model.')\n",
    "\n",
    "cnnmodel = Sequential()\n",
    "cnnmodel.add(embedding_layer)                   # input layer: 500x100\n",
    "\n",
    "cnnmodel.add(Conv1D(128, 5, activation='relu')) # 128 output space; filter size=5; activation function\n",
    "cnnmodel.add(MaxPooling1D(5))                   # max pool size = 5 (maximum value of window size)\n",
    "cnnmodel.add(Dropout(0.25))                     # may avoid overfitting\n",
    "\n",
    "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
    "cnnmodel.add(MaxPooling1D(5))\n",
    "cnnmodel.add(Dropout(0.25))\n",
    "\n",
    "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
    "cnnmodel.add(GlobalMaxPooling1D())\n",
    "cnnmodel.add(Dense(128, activation='relu'))\n",
    "cnnmodel.add(Dropout(0.25))\n",
    "\n",
    "cnnmodel.add(Dense(len(labels_index), activation='softmax')) # classify labels 0 or 1, so is 2; 'softmax' get probability\n",
    "\n",
    "cnnmodel.compile(loss='categorical_crossentropy', # loss function\n",
    "                 optimizer='Adam',\n",
    "                 metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1412, 500)\n",
      "(1412, 2)\n",
      "Epoch 1/5\n",
      "45/45 [==============================] - 5s 97ms/step - loss: 0.6814 - acc: 0.5462 - val_loss: 0.6090 - val_acc: 0.7045\n",
      "Epoch 2/5\n",
      "45/45 [==============================] - 4s 84ms/step - loss: 0.5594 - acc: 0.7161 - val_loss: 0.4874 - val_acc: 0.7898\n",
      "Epoch 3/5\n",
      "45/45 [==============================] - 4s 84ms/step - loss: 0.3744 - acc: 0.8449 - val_loss: 0.4234 - val_acc: 0.8182\n",
      "Epoch 4/5\n",
      "45/45 [==============================] - 4s 83ms/step - loss: 0.3013 - acc: 0.8832 - val_loss: 0.4785 - val_acc: 0.8040\n",
      "Epoch 5/5\n",
      "45/45 [==============================] - 4s 83ms/step - loss: 0.1822 - acc: 0.9411 - val_loss: 0.4636 - val_acc: 0.8324\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "cnn_train = cnnmodel.fit(x_train, y_train, # data, label\n",
    "                         batch_size= 32,   # each time for 32 samples for training set until all samples for 1 epoch.\n",
    "                         epochs = 5,  \n",
    "                         verbose = 1,      # return some informations during training\n",
    "                         validation_data= (x_val, y_val))\n",
    "\n",
    "# epoch , train how many time, low: underfitting, high: overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 2s 28ms/step - loss: 0.1091 - acc: 0.9674\n",
      "Training accuracy with CNN: 0.9674220681190491\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 0.5822 - acc: 0.7831\n",
      "Testing accuracy with CNN: 0.7830687761306763\n"
     ]
    }
   ],
   "source": [
    "score_train, acc_train = cnnmodel.evaluate(x_train, y_train)\n",
    "print('Training accuracy with CNN:', acc_train)\n",
    "\n",
    "score_test, acc_test = cnnmodel.evaluate(test_data, test_labels)\n",
    "print('Testing accuracy with CNN:', acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CNN classification accuracy enhance from 73.2% to 78.3%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model accuracy again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use TF-IDF vectorize again\n",
    "- Fit Logistic and SVM to check accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3.\n",
    "# use new train data(1568 tweets, including original and paraphrase)\n",
    "\n",
    "new_test_data = y_test.to_frame().join(X_test.to_frame())\n",
    "all_new_data = new_train_data.append(new_test_data)\n",
    "\n",
    "X = all_new_data['clean_tweet']\n",
    "y = all_new_data['Is_Unreliable']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf\n",
    "tfidf = TfidfVectorizer(lowercase = True,\n",
    "                        stop_words = 'english',\n",
    "                        ngram_range = (1,1))\n",
    "\n",
    "\n",
    "X_dtm = tfidf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf Logistic Regression Accuracy with 10-folds validation:  0.839\n"
     ]
    }
   ],
   "source": [
    "# tfidf-logistic\n",
    "logreg = LogisticRegression(class_weight=\"balanced\") #instantiate a logistic regression model\n",
    "scores = cross_val_score(logreg, X_dtm, y, cv= 10) # 10 folds validation\n",
    "print( \"tfidf Logistic Regression Accuracy with 10-folds validation: \", round(np.mean(scores),3))\n",
    "\n",
    "a2 = (\"After paraphrasing, tfidf Logistic Regression Accuracy: \"+ str(round(np.mean(scores),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC\n",
    "svc = SVC(probability=True)\n",
    "pipe = Pipeline([\n",
    "('vectorize', tfidf),\n",
    "('classify', SVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = ['rbf', 'linear', 'poly', 'sigmoid']\n",
    "C = [0.001, 0.01, 0.1, 1, 10]\n",
    "# set up parameter grid\n",
    "params = {\n",
    "'classify__kernel': kernel,\n",
    "'classify__C': C\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_cv = KFold(n_splits = 3, shuffle = True, random_state = 1)\n",
    "outer_cv = KFold(n_splits = 10, shuffle = True, random_state = 1)\n",
    "\n",
    "# Set up GridSearch for inner loop\n",
    "grid_SVC = GridSearchCV(pipe, params, cv = inner_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(grid_SVC,\n",
    "                        X = X,\n",
    "                        y = y,\n",
    "                        cv = outer_cv,\n",
    "                        scoring = ['roc_auc', 'accuracy', 'f1', 'precision', 'recall'],\n",
    "                        return_estimator = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.91269841 0.86904762 0.90079365 0.92460317 0.93253968 0.92063492\n",
      " 0.91666667 0.92857143 0.92063492 0.88492063]\n",
      "\n",
      "After paraphrasing, tfidf SVC Accuracy: 0.911\n"
     ]
    }
   ],
   "source": [
    "accuracy = scores['test_accuracy']\n",
    "print(accuracy)\n",
    "b2 = (\"After paraphrasing, tfidf SVC Accuracy: \" +str(round(accuracy.mean(),3)))\n",
    "print(\"\\n\"+ b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf Logistic Regression Accuracy: 0.812\n",
      "After paraphrasing, tfidf Logistic Regression Accuracy: 0.839\n",
      "\n",
      "tfidf SVC Accuracy: 0.82\n",
      "After paraphrasing, tfidf SVC Accuracy: 0.911\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(a2 + \"\\n\")\n",
    "\n",
    "print(b)\n",
    "print(b2 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Indeed, our accuracy get better after we parapharse our tweets to expand dataset. Logistic from 81.2% to 83.9%; SVM from 82% to 91%. However, if we want to get more better accuracy, we probably need to create new sentence with different structure. Because I only change words on my work, the structure is still same. If we change the words and structure to create new sentences, the training model can learn more information and get more improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
